---
layout: post
comments: true
author: Silvano Gai
title: The Pendolum of Domain-Specific Hardware
excerpt: A brief description of the popularity of domain Specific hardware and the reasons behind it
description: Domain-specific hardware vs. general purpose processors
---
There is a lot of talks these days about "domain-specific hardware." It is essential to have a historical perspective to understand this discussion because the pendulum has swung in both directions a couple of times.

The concept of domain-specific hardware is not new. Back in the 1970s–1980s, at the time of mainframes, the central CPUs were not fast enough to do both computing and I/O. Coprocessors and I/O offloads were common. During the years 1990 to 2000, we all witnessed a rapid growth of processor performance associated with significant innovation in the processor architecture, namely pipelined processor, superscalar processor, speculative execution, and virtualization extensions for speeding up hypervisors. Integrated caches played a big part in this performance boost as well, because processor speeds grew super-linearly as compared to DRAM speeds. All this evolution happened thanks to the capability to shrink the transistor size. In the 2000s, with transistor channel-length becoming sub-micron, it became challenging to continue to increase the frequency because wires were becoming the dominant delay element. The only effective way to utilize the growing number of transistors was with a multicore architecture; that is, putting multiple CPUs on the same chip. At that point, the increase in the speed of the single CPU (or more appropriately core) slowed down significantly. The SpecINT benchmark that tracks the single-thread performance started to grow only single-digit per year. In 2018, single-thread CPU speed only grew approximately 4 percent per year.  

![Book Cover](/assets/images/specint.png)

The slowdown in Moore's law, the end of Dennard scaling, and other technical issues related to the difficulty of further reducing the size of the transistors are causing this slowdown. We will cover them in a future post.

In 2010 the pendulum started to swing back. With the introduction of the "cloud," the cloud providers began to realize that the amount of I/O traffic was growing extremely fast due to the explosion of East-West traffic, storage networking, and applications that are I/O intensive. Implementing network, storage, and security services in the server CPU was consuming too many CPU cycles — these precious CPU cycles needed to be used and billed to users, not to services.
The historical period with the mindset: "*... we don't need specialized hardware since CPU speeds continue increasing, CPUs are general-purpose, the software is easy to write …*" came to an end with a renewed interest in domain-specific hardware.

Domain-specific hardware is optimized for certain types of data processing tasks, allowing it to perform those tasks way faster and more efficiently than general-purpose CPUs. For example, graphics processing units (GPUs) are an example of domain-specific hardware. Created to support advanced graphics interfaces, today they are also extensively used for artificial intelligence and machine learning. The key to GPUs' success is the highly parallel structure that makes them more efficient than CPUs for algorithms that process large blocks of data in parallel and rely on extensive floating-point math computations.

This topic is covered in detail in Chapter 7 of my book [Building a Future-Proof Cloud Infrastructure: A Unified Architecture for Network, Security, and Storage Services](http://www.informit.com/store/distributed-services-architectureseffectively-deploying-9780136624097).
